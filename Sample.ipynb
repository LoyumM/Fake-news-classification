{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Importing libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import re\n",
    "import nltk\n",
    "import itertools\n",
    "from sklearn import metrics\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from tensorflow.keras.layers import Embedding\n",
    "from tensorflow.keras.preprocessing.text import one_hot\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.layers import Bidirectional\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.metrics import accuracy_score\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import classification_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\Loyumba\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\", \"you've\", \"you'll\", \"you'd\", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', \"she's\", 'her', 'hers', 'herself', 'it', \"it's\", 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', \"that'll\", 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', \"don't\", 'should', \"should've\", 'now', 'd', 'll', 'm', 'o', 're', 've', 'y', 'ain', 'aren', \"aren't\", 'couldn', \"couldn't\", 'didn', \"didn't\", 'doesn', \"doesn't\", 'hadn', \"hadn't\", 'hasn', \"hasn't\", 'haven', \"haven't\", 'isn', \"isn't\", 'ma', 'mightn', \"mightn't\", 'mustn', \"mustn't\", 'needn', \"needn't\", 'shan', \"shan't\", 'shouldn', \"shouldn't\", 'wasn', \"wasn't\", 'weren', \"weren't\", 'won', \"won't\", 'wouldn', \"wouldn't\"]\n"
     ]
    }
   ],
   "source": [
    "print(stopwords.words('english'))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocessing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"data.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>title</th>\n",
       "      <th>author</th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>House Dem Aide: We Didn’t Even See Comey’s Let...</td>\n",
       "      <td>Darrell Lucus</td>\n",
       "      <td>House Dem Aide: We Didn’t Even See Comey’s Let...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>FLYNN: Hillary Clinton, Big Woman on Campus - ...</td>\n",
       "      <td>Daniel J. Flynn</td>\n",
       "      <td>Ever get the feeling your life circles the rou...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>Why the Truth Might Get You Fired</td>\n",
       "      <td>Consortiumnews.com</td>\n",
       "      <td>Why the Truth Might Get You Fired October 29, ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>15 Civilians Killed In Single US Airstrike Hav...</td>\n",
       "      <td>Jessica Purkiss</td>\n",
       "      <td>Videos 15 Civilians Killed In Single US Airstr...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>Iranian woman jailed for fictional unpublished...</td>\n",
       "      <td>Howard Portnoy</td>\n",
       "      <td>Print \\r\\nAn Iranian woman has been sentenced ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id                                              title              author  \\\n",
       "0   0  House Dem Aide: We Didn’t Even See Comey’s Let...       Darrell Lucus   \n",
       "1   1  FLYNN: Hillary Clinton, Big Woman on Campus - ...     Daniel J. Flynn   \n",
       "2   2                  Why the Truth Might Get You Fired  Consortiumnews.com   \n",
       "3   3  15 Civilians Killed In Single US Airstrike Hav...     Jessica Purkiss   \n",
       "4   4  Iranian woman jailed for fictional unpublished...      Howard Portnoy   \n",
       "\n",
       "                                                text  label  \n",
       "0  House Dem Aide: We Didn’t Even See Comey’s Let...      1  \n",
       "1  Ever get the feeling your life circles the rou...      0  \n",
       "2  Why the Truth Might Get You Fired October 29, ...      1  \n",
       "3  Videos 15 Civilians Killed In Single US Airstr...      1  \n",
       "4  Print \\r\\nAn Iranian woman has been sentenced ...      1  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(20800, 5)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "id           0\n",
       "title      558\n",
       "author    1957\n",
       "text        39\n",
       "label        0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.isnull().sum()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As these are text data, I've decided to fill the null values with empty strings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "id        0\n",
       "title     0\n",
       "author    0\n",
       "text      0\n",
       "label     0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = df.fillna(\"\")\n",
    "df.isnull().sum()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I've planned to combine the author and title for predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# merging the author name and news title\n",
    "df['content'] = df['author']+' '+df['title']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    Darrell Lucus House Dem Aide: We Didn’t Even S...\n",
       "1    Daniel J. Flynn FLYNN: Hillary Clinton, Big Wo...\n",
       "2    Consortiumnews.com Why the Truth Might Get You...\n",
       "3    Jessica Purkiss 15 Civilians Killed In Single ...\n",
       "4    Howard Portnoy Iranian woman jailed for fictio...\n",
       "Name: content, dtype: object"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['content'].head()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Seperating data and label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(20800, 5)\n",
      "(20800,)\n"
     ]
    }
   ],
   "source": [
    "# getting the independent features\n",
    "X = df.drop('label', axis=1)\n",
    "# getting the dependent feature\n",
    "y = df['label']\n",
    "\n",
    "print(X.shape)\n",
    "print(y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Daniel J. Flynn FLYNN: Hillary Clinton, Big Woman on Campus - Breitbart'"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "messages=X.copy()\n",
    "messages['content'][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>title</th>\n",
       "      <th>author</th>\n",
       "      <th>text</th>\n",
       "      <th>content</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>House Dem Aide: We Didn’t Even See Comey’s Let...</td>\n",
       "      <td>Darrell Lucus</td>\n",
       "      <td>House Dem Aide: We Didn’t Even See Comey’s Let...</td>\n",
       "      <td>Darrell Lucus House Dem Aide: We Didn’t Even S...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>FLYNN: Hillary Clinton, Big Woman on Campus - ...</td>\n",
       "      <td>Daniel J. Flynn</td>\n",
       "      <td>Ever get the feeling your life circles the rou...</td>\n",
       "      <td>Daniel J. Flynn FLYNN: Hillary Clinton, Big Wo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>Why the Truth Might Get You Fired</td>\n",
       "      <td>Consortiumnews.com</td>\n",
       "      <td>Why the Truth Might Get You Fired October 29, ...</td>\n",
       "      <td>Consortiumnews.com Why the Truth Might Get You...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>15 Civilians Killed In Single US Airstrike Hav...</td>\n",
       "      <td>Jessica Purkiss</td>\n",
       "      <td>Videos 15 Civilians Killed In Single US Airstr...</td>\n",
       "      <td>Jessica Purkiss 15 Civilians Killed In Single ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>Iranian woman jailed for fictional unpublished...</td>\n",
       "      <td>Howard Portnoy</td>\n",
       "      <td>Print \\r\\nAn Iranian woman has been sentenced ...</td>\n",
       "      <td>Howard Portnoy Iranian woman jailed for fictio...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id                                              title              author  \\\n",
       "0   0  House Dem Aide: We Didn’t Even See Comey’s Let...       Darrell Lucus   \n",
       "1   1  FLYNN: Hillary Clinton, Big Woman on Campus - ...     Daniel J. Flynn   \n",
       "2   2                  Why the Truth Might Get You Fired  Consortiumnews.com   \n",
       "3   3  15 Civilians Killed In Single US Airstrike Hav...     Jessica Purkiss   \n",
       "4   4  Iranian woman jailed for fictional unpublished...      Howard Portnoy   \n",
       "\n",
       "                                                text  \\\n",
       "0  House Dem Aide: We Didn’t Even See Comey’s Let...   \n",
       "1  Ever get the feeling your life circles the rou...   \n",
       "2  Why the Truth Might Get You Fired October 29, ...   \n",
       "3  Videos 15 Civilians Killed In Single US Airstr...   \n",
       "4  Print \\r\\nAn Iranian woman has been sentenced ...   \n",
       "\n",
       "                                             content  \n",
       "0  Darrell Lucus House Dem Aide: We Didn’t Even S...  \n",
       "1  Daniel J. Flynn FLYNN: Hillary Clinton, Big Wo...  \n",
       "2  Consortiumnews.com Why the Truth Might Get You...  \n",
       "3  Jessica Purkiss 15 Civilians Killed In Single ...  \n",
       "4  Howard Portnoy Iranian woman jailed for fictio...  "
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "messages.head()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocessing and stemming\n",
    "\n",
    "Stemming is an algorithm in linguistic normalization where each words are reduced to its root word.\n",
    "\n",
    "Example:\n",
    "(i) History, Historical => Histori\n",
    "(ii) Eat, eating, eaten => Eat\n",
    "\n",
    "Note: Some of the root word loses its meaning as seen in example (i). This shouldn't be an issue for news classification, but in cases where the word meaning needs to be retained use lemmetization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset Preprocessing\n",
    "ps = PorterStemmer()\n",
    "corpus = []\n",
    "for i in range(len(messages)):\n",
    "    review = re.sub('[^a-zA-Z]', ' ', messages['content'][i])\n",
    "    review = review.lower()\n",
    "    review = review.split()\n",
    "    \n",
    "    review = [ps.stem(word) for word in review if not word in stopwords.words('english')]\n",
    "    review = ' '.join(review)\n",
    "    corpus.append(review)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['daniel j flynn flynn hillari clinton big woman campu breitbart',\n",
       " 'consortiumnew com truth might get fire',\n",
       " 'jessica purkiss civilian kill singl us airstrik identifi',\n",
       " 'howard portnoy iranian woman jail fiction unpublish stori woman stone death adulteri',\n",
       " 'daniel nussbaum jacki mason hollywood would love trump bomb north korea lack tran bathroom exclus video breitbart',\n",
       " 'life life luxuri elton john favorit shark pictur stare long transcontinent flight',\n",
       " 'alissa j rubin beno hamon win french socialist parti presidenti nomin new york time',\n",
       " 'excerpt draft script donald trump q ampa black church pastor new york time',\n",
       " 'megan twohey scott shane back channel plan ukrain russia courtesi trump associ new york time']"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# previewing the first 10 sentences reduced to its root word\n",
    "corpus[1:10]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### One hot encodding\n",
    "I've decided to try bidirectional LSTM by one hot encoding and other machine learning algorithms by TF-IDF vectorising\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[1790, 4720, 6909, 6909, 3986, 8722, 7467, 7010, 448, 9865],\n",
       " [1803, 372, 5288, 1382, 5324, 7391]]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## one hot representation\n",
    "voc_size=10000\n",
    "onehot_repr=[one_hot(words,voc_size)for words in corpus] \n",
    "onehot_repr[1:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The first sentence after stemming looks like: darrel lucu hous dem aid even see comey letter jason chaffetz tweet\n",
      "The same sentence after one hot encoding looks like: [2270, 4972, 6139, 1888, 2950, 12, 4590, 1752, 7934, 3968, 7197, 6072]\n"
     ]
    }
   ],
   "source": [
    "print(f\"The first sentence after stemming looks like: {corpus[0]}\")\n",
    "print(f\"The same sentence after one hot encoding looks like: {onehot_repr[0]}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Embedding representation\n",
    "\n",
    "Padding sentence to make them of the same size. We can use 'pre' or 'post' padding, but using one instead of the other didn't make any significant improvements to the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[2270 4972 6139 ...    0    0    0]\n",
      " [1790 4720 6909 ...    0    0    0]\n",
      " [1803  372 5288 ...    0    0    0]\n",
      " ...\n",
      " [9221 4720 5772 ...    0    0    0]\n",
      " [7394 1745 4721 ...    0    0    0]\n",
      " [ 131 7813 8586 ...    0    0    0]]\n"
     ]
    }
   ],
   "source": [
    "sent_length=25\n",
    "embedded_docs=pad_sequences(onehot_repr,padding='post',maxlen=sent_length)\n",
    "print(embedded_docs)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " embedding (Embedding)       (None, 25, 40)            400000    \n",
      "                                                                 \n",
      " bidirectional (Bidirectiona  (None, 200)              112800    \n",
      " l)                                                              \n",
      "                                                                 \n",
      " dense (Dense)               (None, 1)                 201       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 513,001\n",
      "Trainable params: 513,001\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "## creating model\n",
    "embedded_vector_features=40#number of features\n",
    "model=Sequential()\n",
    "model.add(Embedding(voc_size,embedded_vector_features,input_length=sent_length))\n",
    "model.add(Bidirectional(LSTM(100)))#number of nuerons\n",
    "model.add(Dense(1,activation='sigmoid'))# as it is binary classifier\n",
    "model.compile(loss='binary_crossentropy',optimizer='adam',metrics=['accuracy'])\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# converting into numpy array\n",
    "X_final=np.array(embedded_docs)\n",
    "y_final=np.array(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((20800, 25), (20800,))"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_final.shape,y_final.shape"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Splitting into training and test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X_final, y_final, test_size=0.3, random_state=33)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "228/228 [==============================] - 8s 37ms/step - loss: 5.5830e-06 - accuracy: 1.0000 - val_loss: 0.1071 - val_accuracy: 0.9846\n",
      "Epoch 2/10\n",
      "228/228 [==============================] - 8s 36ms/step - loss: 4.7606e-06 - accuracy: 1.0000 - val_loss: 0.1098 - val_accuracy: 0.9845\n",
      "Epoch 3/10\n",
      "228/228 [==============================] - 8s 35ms/step - loss: 4.0827e-06 - accuracy: 1.0000 - val_loss: 0.1125 - val_accuracy: 0.9841\n",
      "Epoch 4/10\n",
      "228/228 [==============================] - 9s 40ms/step - loss: 3.5192e-06 - accuracy: 1.0000 - val_loss: 0.1153 - val_accuracy: 0.9843\n",
      "Epoch 5/10\n",
      "228/228 [==============================] - 9s 40ms/step - loss: 3.0369e-06 - accuracy: 1.0000 - val_loss: 0.1179 - val_accuracy: 0.9841\n",
      "Epoch 6/10\n",
      "228/228 [==============================] - 9s 41ms/step - loss: 2.6257e-06 - accuracy: 1.0000 - val_loss: 0.1202 - val_accuracy: 0.9841\n",
      "Epoch 7/10\n",
      "228/228 [==============================] - 9s 41ms/step - loss: 2.2828e-06 - accuracy: 1.0000 - val_loss: 0.1228 - val_accuracy: 0.9840\n",
      "Epoch 8/10\n",
      "228/228 [==============================] - 10s 42ms/step - loss: 1.9916e-06 - accuracy: 1.0000 - val_loss: 0.1252 - val_accuracy: 0.9840\n",
      "Epoch 9/10\n",
      "228/228 [==============================] - 10s 42ms/step - loss: 1.7332e-06 - accuracy: 1.0000 - val_loss: 0.1275 - val_accuracy: 0.9840\n",
      "Epoch 10/10\n",
      "228/228 [==============================] - 10s 43ms/step - loss: 1.5162e-06 - accuracy: 1.0000 - val_loss: 0.1297 - val_accuracy: 0.9837\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1f2087f9330>"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "##model training\n",
    "model.fit(X_train, y_train, validation_data=(X_test, y_test),epochs=10, batch_size=64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "195/195 [==============================] - 1s 7ms/step\n"
     ]
    }
   ],
   "source": [
    "y_pred=model.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred=np.where(y_pred>=0.5,1,0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[3096   14]\n",
      " [  88 3042]]\n",
      "Accuracy is 0.9836538461538461\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.97      1.00      0.98      3110\n",
      "           1       1.00      0.97      0.98      3130\n",
      "\n",
      "    accuracy                           0.98      6240\n",
      "   macro avg       0.98      0.98      0.98      6240\n",
      "weighted avg       0.98      0.98      0.98      6240\n",
      "\n"
     ]
    }
   ],
   "source": [
    "conf_mat = confusion_matrix(y_test,y_pred)\n",
    "acc = accuracy_score(y_test,y_pred)\n",
    "class_rep = classification_report(y_test,y_pred)\n",
    "\n",
    "print(conf_mat)\n",
    "print(f'Accuracy is {acc}')\n",
    "print(class_rep)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Vectorizing the data\n",
    "\n",
    "Now we try vectorizing the data and using machine learning models\n",
    "\n",
    "TF-IDF measures the importance of words in a document\n",
    "- Term Frequency is the number of times the word appears in the document.\n",
    "- Inverse Document Frequency measures the rarity of the word in the document."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['daniel j flynn flynn hillari clinton big woman campu breitbart',\n",
       " 'consortiumnew com truth might get fire',\n",
       " 'jessica purkiss civilian kill singl us airstrik identifi',\n",
       " 'howard portnoy iranian woman jail fiction unpublish stori woman stone death adulteri',\n",
       " 'daniel nussbaum jacki mason hollywood would love trump bomb north korea lack tran bathroom exclus video breitbart',\n",
       " 'life life luxuri elton john favorit shark pictur stare long transcontinent flight',\n",
       " 'alissa j rubin beno hamon win french socialist parti presidenti nomin new york time',\n",
       " 'excerpt draft script donald trump q ampa black church pastor new york time',\n",
       " 'megan twohey scott shane back channel plan ukrain russia courtesi trump associ new york time']"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corpus[1:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "20800"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0        1\n",
       "1        0\n",
       "2        1\n",
       "3        1\n",
       "4        1\n",
       "        ..\n",
       "20795    0\n",
       "20796    0\n",
       "20797    0\n",
       "20798    1\n",
       "20799    1\n",
       "Name: label, Length: 20800, dtype: int64"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# converting the textual data into numerical data \n",
    "vectorizer = TfidfVectorizer()\n",
    "vectorizer.fit(corpus)\n",
    "X = vectorizer.transform(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  (0, 16799)\t0.30071745655510157\n",
      "  (0, 6816)\t0.1904660198296849\n",
      "  (0, 5503)\t0.7143299355715573\n",
      "  (0, 3568)\t0.26373768806048464\n",
      "  (0, 2813)\t0.19094574062359204\n",
      "  (0, 2223)\t0.3827320386859759\n",
      "  (0, 1894)\t0.15521974226349364\n",
      "  (0, 1497)\t0.2939891562094648\n",
      "  (1, 15611)\t0.41544962664721613\n",
      "  (1, 9620)\t0.49351492943649944\n",
      "  (1, 5968)\t0.3474613386728292\n",
      "  (1, 5389)\t0.3866530551182615\n",
      "  (1, 3103)\t0.46097489583229645\n",
      "  (1, 2943)\t0.3179886800654691\n"
     ]
    }
   ],
   "source": [
    "print(X[1:3])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Splitting the vectorized data into train and test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.3, stratify=y, random_state=33)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = LogisticRegression()\n",
    "model.fit(X_train, y_train)\n",
    "y_pred = model.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[2989  127]\n",
      " [  24 3100]]\n",
      "Accuracy score of the training data : 0.975801282051282\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.99      0.96      0.98      3116\n",
      "           1       0.96      0.99      0.98      3124\n",
      "\n",
      "    accuracy                           0.98      6240\n",
      "   macro avg       0.98      0.98      0.98      6240\n",
      "weighted avg       0.98      0.98      0.98      6240\n",
      "\n"
     ]
    }
   ],
   "source": [
    "conf_mat = confusion_matrix(y_test,y_pred)\n",
    "acc = accuracy_score(y_test,y_pred)\n",
    "class_rep = classification_report(y_test,y_pred)\n",
    "\n",
    "print(conf_mat)\n",
    "print(f'Accuracy score of the training data : {acc}')\n",
    "print(class_rep)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Multinomial Naive Bayes\n",
    "\n",
    "Multinomial Naïve Bayes uses term frequency i.e. the number of times a given term appears in a document. Term frequency is often normalized by dividing the raw term frequency by the document length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.3, random_state=33)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier=MultinomialNB()\n",
    "classifier.fit(X_train, y_train)\n",
    "y_pred = classifier.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[3090   20]\n",
      " [ 268 2862]]\n",
      "Accuracy score of the training data : 0.9538461538461539\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.92      0.99      0.96      3110\n",
      "           1       0.99      0.91      0.95      3130\n",
      "\n",
      "    accuracy                           0.95      6240\n",
      "   macro avg       0.96      0.95      0.95      6240\n",
      "weighted avg       0.96      0.95      0.95      6240\n",
      "\n"
     ]
    }
   ],
   "source": [
    "conf_mat = confusion_matrix(y_test,y_pred)\n",
    "acc = accuracy_score(y_test,y_pred)\n",
    "class_rep = classification_report(y_test,y_pred)\n",
    "\n",
    "print(conf_mat)\n",
    "print(f'Accuracy score of the training data : {acc}')\n",
    "print(class_rep)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MODEL COMPARISON\n",
    "\n",
    "In this dataset we can observe that:\n",
    "\n",
    "(i) With one hot encodiding:\n",
    "\n",
    "- Bidirectional LSTM: Accuracy is: 98.36 %\n",
    "\n",
    "(ii) Using TF-IDF to vectorize the data:\n",
    "\n",
    "- Logistic Regression : Accuracy is 97.58 %\n",
    "\n",
    "- Multinomial Naive Bayes : Accuracy is 95.38 %\n",
    "\n",
    "### CONCLUSION \n",
    "\n",
    "- Bidirectional LSTM with one hot encoding and embedding works great.\n",
    "- We can also try vectorizing the data using word2vec"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "49fd5712af1c3ece0edd15a96db780ec3225b5f1f993546e31e22d59c14f183a"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
